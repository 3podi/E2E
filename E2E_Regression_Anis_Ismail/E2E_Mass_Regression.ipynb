{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GSOC_E2E_Mass_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q7-y3-6AweZ"
      },
      "source": [
        "# End-to-End Mass Regression for Boosted Top Quarks \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKr0rHe2uIcn"
      },
      "source": [
        "## Importing the Needed Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfCXAmy4SK8l"
      },
      "source": [
        "import numpy as np\n",
        "import os, glob\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LogNorm\n",
        "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
        "plt.switch_backend('agg')\n",
        "%matplotlib inline\n",
        "\n",
        "import random\n",
        "import gc"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuSceJtsuZJt"
      },
      "source": [
        "##Defining the Resnet-15 Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjV59z5V8LSt"
      },
      "source": [
        "class ResBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.downsample = out_channels//in_channels\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=self.downsample, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=self.downsample)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        if self.downsample > 1:\n",
        "            residual = self.shortcut(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, nblocks, fmaps):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.fmaps = fmaps\n",
        "        self.nblocks = nblocks\n",
        "\n",
        "        self.conv0 = nn.Conv2d(in_channels, fmaps[0], kernel_size=7, stride=1, padding=1)\n",
        "        self.layer1 = self.block_layers(self.nblocks, [fmaps[0],fmaps[0]])\n",
        "        self.layer2 = self.block_layers(1, [fmaps[0],fmaps[1]])\n",
        "        self.layer3 = self.block_layers(self.nblocks, [fmaps[1],fmaps[1]])\n",
        "\n",
        "        # no FC\n",
        "        self.fc = nn.Linear(self.fmaps[1]+2, 1)\n",
        "\n",
        "        self.GlobalMaxPool2d = nn.AdaptiveMaxPool2d((1,1))\n",
        "        \n",
        "    def block_layers(self, nblocks, fmaps):\n",
        "        layers = []\n",
        "        for _ in range(nblocks):\n",
        "            layers.append(ResBlock(fmaps[0], fmaps[1]))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        x = self.conv0(X[0])\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        \n",
        "        x = self.GlobalMaxPool2d(x)\n",
        "        x = x.view(x.size()[0], self.fmaps[1])\n",
        "        # concat with seed pos\n",
        "        x = torch.cat([x, X[1], X[2]], 1)\n",
        "        # FC\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBQwXkAvvNnP"
      },
      "source": [
        "##Defining the Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw6jMi-m4_iI"
      },
      "source": [
        "def mae_loss_wgtd(pred, true, is_cuda, wgt=1.):\n",
        "    loss = wgt*(pred-true).abs()\n",
        "    if is_cuda: \n",
        "        loss = loss.cuda()\n",
        "    return loss.mean()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMXF0uRavR1G"
      },
      "source": [
        "##Define the Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZisjNjfvSI4"
      },
      "source": [
        "def transform_y(y, m0_scale = 500):\n",
        "    return y/m0_scale\n",
        "\n",
        "def inv_transform(y, m0_scale = 500):\n",
        "    return y*m0_scale\n",
        "\n",
        "class ParquetDataset(Dataset):\n",
        "    def __init__(self, filename, label):\n",
        "        self.parquet = pq.ParquetFile(filename)\n",
        "        self.cols = [\"X_jet\",\"genM\",\"iphi\",\"ieta\"]\n",
        "        self.label = label\n",
        "    def __getitem__(self, index):\n",
        "        data = self.parquet.read_row_group(index,columns=self.cols).to_pandas()\n",
        "        data_dict={}\n",
        "        data_dict['X_jet'] = np.array([[[coord for coord in xk] for xk in xj] for xj in data[\"X_jet\"].values[0]], ndmin=3,dtype=np.float32) \n",
        "        data_dict['X_jet'][0] = channel1_scale * data_dict['X_jet'][0] \n",
        "        data_dict['X_jet'][1] = channel2_scale * data_dict['X_jet'][1] \n",
        "        data_dict['X_jet'][2] = channel3_scale   * data_dict['X_jet'][2] \n",
        "        data_dict['genM'] = transform_y(np.float32(data['genM'].values))\n",
        "        data_dict['iphi'] = np.float32(data['iphi'].values)/360.\n",
        "        data_dict['ieta'] = np.float32(data['ieta'].values)/170.\n",
        "        # Zero-Suppression\n",
        "        data_dict['X_jet'][data_dict['X_jet'] < 1.e-3] = 0. \n",
        "        # High Value Suppression\n",
        "        data_dict['X_jet'][0][data_dict['X_jet'][0] > 50] = 1. \n",
        "        data_dict['X_jet'][1][data_dict['X_jet'][1] > 5] = 1. \n",
        "        data_dict['X_jet'][2][data_dict['X_jet'][2] > 5] = 1. \n",
        "        return data_dict\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.parquet.num_row_groups\n",
        "       \n",
        "def train_val_loader(datasets, batch_size, random_sampler=True):\n",
        "    dset = ConcatDataset([ParquetDataset(dataset, datasets.index(dataset)) for dataset in datasets])\n",
        "    idxs = np.random.permutation(len(dset))\n",
        "    print(len(dset))\n",
        "    train_cut = int(len(dset) * 0.9)\n",
        "    val_cut = int(len(dset) * 0.05)\n",
        "    if random_sampler: \n",
        "        train_sampler = SubsetRandomSampler(idxs[:train_cut])\n",
        "        val_sampler = SubsetRandomSampler(idxs[train_cut:train_cut+val_cut])\n",
        "        test_sampler = SubsetRandomSampler(idxs[train_cut+val_cut:])\n",
        "    else: \n",
        "        train_sampler, val_sampler = None, None \n",
        "    train_loader = DataLoader(dataset=dset, batch_size=batch_size, shuffle=False, num_workers=4, sampler=train_sampler, pin_memory=True)\n",
        "    val_loader = DataLoader(dataset=dset, batch_size=batch_size, shuffle=False, num_workers=4, sampler=val_sampler, pin_memory=True)\n",
        "    test_loader = DataLoader(dataset=dset, batch_size=batch_size, shuffle=False, num_workers=4, sampler=test_sampler, pin_memory=True)\n",
        "    return train_loader, val_loader,test_loader"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY1rQvwVwVLD"
      },
      "source": [
        "##Defining Train and Evaluation Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMxaHwLxv5f9"
      },
      "source": [
        "def logger(s, f, run_logger=True):\n",
        "    print(s)\n",
        "    if run_logger:\n",
        "        f.write('%s\\n' % str(s))\n",
        "        \n",
        "def load_model(model_name, resnet, optimizer, lr_scheduler):\n",
        "    print(model_name)\n",
        "    checkpoint = torch.load(os.path.join(params[\"save_path\"], \"MODELS\") + \"/%s/%s \" % (params[\"expt_name\"], model_name))\n",
        "    resnet.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    return resnet, optimizer, lr_scheduler\n",
        "\n",
        "def do_eval(resnet, val_loader, epoch):\n",
        "    mass_bins = np.arange(85, 500, 25)  # for histogram in eval()\n",
        "    loss_ = 0.\n",
        "    m_pred_, m_true_, mae_, mre_ = [], [], [], []\n",
        "    now = time.time()\n",
        "    ma_low = transform_y(85.0)  # convert from GeV to network units\n",
        "    for i, data in enumerate(val_loader):\n",
        "        X, m0, iphi, ieta = data['X_jet'].cuda(), data['genM'].cuda(), data['iphi'].cuda(), data['ieta'].cuda()\n",
        "        X = X[m0[:, 0] > ma_low]\n",
        "        iphi = iphi[m0[:, 0] > ma_low]\n",
        "        ieta = ieta[m0[:, 0] > ma_low]\n",
        "        m0 = m0[m0[:, 0] > ma_low]\n",
        "        logits = resnet([X, iphi, ieta])\n",
        "        loss_ += mae_loss_wgtd(logits, m0, is_cuda=params[\"is_cuda\"]).item()\n",
        "\n",
        "        # Undo preprocessing on mass\n",
        "        logits, m0 = inv_transform(logits), inv_transform(m0)\n",
        "        mae = (logits - m0).abs()\n",
        "        mre = ((logits - m0).abs() / m0)\n",
        "        # Store batch metrics:\n",
        "\n",
        "        m_pred_.append(logits.tolist())\n",
        "        m_true_.append(m0.tolist())\n",
        "        mae_.append(mae.tolist())\n",
        "        mre_.append(mre.tolist())\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "    now = time.time() - now\n",
        "    m_true_ = np.concatenate(m_true_)\n",
        "    m_pred_ = np.concatenate(m_pred_)\n",
        "    mae_ = np.concatenate(mae_)\n",
        "    mre_ = np.concatenate(mre_)\n",
        "\n",
        "    logger('%d: Val m_pred: %s... ' % (epoch, str(np.squeeze(m_pred_[:5]))), f, params[\"run_logger\"])\n",
        "    logger('%d: Val m_true: %s... ' % (epoch, str(np.squeeze(m_true_[:5]))), f, params[\"run_logger\"])\n",
        "    logger('%d: Val time:%.2fs in %d steps for N=%d ' % (epoch, now, len(val_loader), len(m_true_)), f,\n",
        "           params[\"run_logger\"])\n",
        "    logger('%d: Val loss:%f, mae:%f, mre:%f ' % (epoch, loss_ / len(val_loader), np.mean(mae_), np.mean(mre_)), f,\n",
        "           params[\"run_logger\"])\n",
        "\n",
        "    score_str = 'epoch%d_mae%.4f ' % (epoch, np.mean(mae_))\n",
        "\n",
        "    # Check 1D m_pred\n",
        "    hst = np.histogram(np.squeeze(m_pred_), bins=mass_bins)[0]\n",
        "    logger('%d: Val m_pred, [85,500,25] MeV: %s ' % (epoch, str(np.uint(hst))), f, params[\"run_logger\"])\n",
        "    mlow = hst[0]\n",
        "    mrms = np.std(hst)\n",
        "    logger('%d: Val m_pred, [85,500,25] MeV: low:%d, rms: %f ' % (epoch, mlow, mrms), f, params[\"run_logger\"])\n",
        "    plt.hist(m_true_, range=(80, 510), bins=20, histtype='step', label=r'$\\mathrm{m_{true}}$', linestyle='--',\n",
        "             color='grey', alpha=0.6)\n",
        "    plt.hist(m_pred_, range=(80, 510), bins=20, histtype='step', label=r'$\\mathrm{m_{pred}}$', linestyle='--',\n",
        "             color='C0', alpha=0.6)\n",
        "    plt.xlim(80, 510)\n",
        "    plt.xlabel(r'$\\mathrm{m}$', size=16)\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.savefig(\n",
        "        os.path.join(params[\"save_path\"], \"PLOTS\") + '/%s/test_mpred_%s.png' % (params[\"expt_name\"], score_str),\n",
        "        bbox_inches='tight')\n",
        "    plt.close()\n",
        "    return mre_, m_pred_, m_true_, np.mean(mae_)\n",
        "\n",
        "\n",
        "def train(resnet, optimizer, lr_scheduler, epochs, train_loader, val_loader):\n",
        "    if params[\"load_epoch\"] != 0:\n",
        "        model_name = 'Tops_ResNet_blocks_3_model_epoch_%d ' % (params[\"load_epoch\"])\n",
        "        resnet, optimizer, lr_scheduler = load_model(model_name, resnet, optimizer, lr_scheduler)\n",
        "    print_step = 100\n",
        "    resnet.train()\n",
        "    for e in range(epochs):\n",
        "        resnet.train()\n",
        "        global f\n",
        "        f = open(os.path.join(params[\"save_path\"], \"LOGS\") + '/%s.log ' % (params[\"expt_name\"]), 'a')\n",
        "        epoch = e + 1 + params[\"load_epoch\"]\n",
        "        n_trained = 0\n",
        "        loss_ = 0.\n",
        "        logger('>> Epoch %d <<<<<<<<' % (epoch), f)\n",
        "\n",
        "        # Run training\n",
        "        resnet.train()\n",
        "        now = time.time()\n",
        "        for i, data in enumerate(train_loader):\n",
        "            X, m0, iphi, ieta = data['X_jet'].cuda(), data['genM'].cuda(), data['iphi'].cuda(), data['ieta'].cuda()\n",
        "            optimizer.zero_grad()\n",
        "            logits = resnet([X, iphi, ieta])\n",
        "            loss = mae_loss_wgtd(logits, m0, is_cuda=params[\"is_cuda\"])\n",
        "            loss.backward()\n",
        "            loss_ += loss.item()\n",
        "            optimizer.step()\n",
        "\n",
        "            n_trained += 1\n",
        "\n",
        "            if i % print_step == 0:\n",
        "                logits, m0 = inv_transform(logits), inv_transform(m0)\n",
        "                mae = (logits - m0).abs().mean()\n",
        "                mre = ((logits - m0).abs() / m0).mean()\n",
        "                logger('%d: (%d/%d) m_pred: %s' % (epoch, i, len(train_loader), str(np.squeeze(logits.tolist()[:5]))),\n",
        "                       f, params[\"run_logger\"])\n",
        "                logger('%d: (%d/%d) m_true: %s' % (epoch, i, len(train_loader), str(np.squeeze(m0.tolist()[:5]))), f,\n",
        "                       params[\"run_logger\"])\n",
        "                logger('%d: (%d/%d) Train loss: %f, mae: %f,mre: %f' % (\n",
        "                    epoch, i, len(train_loader), loss.item(), mae.item(), mre.item()), f, params[\"run_logger\"])\n",
        "            gc.collect()\n",
        "\n",
        "        now = time.time() - now\n",
        "        logits, m0 = inv_transform(logits), inv_transform(m0)\n",
        "        mae = (logits - m0).abs().mean()\n",
        "        mre = ((logits - m0).abs() / m0).mean()\n",
        "        logger('%d: Train time: %.2fs in %d steps for N:%d' % (epoch, now, len(train_loader), n_trained), f,\n",
        "               params[\"run_logger\"])\n",
        "        logger('%d: Average Train loss: %f, Final mae: %f,Final mre: %f' % (\n",
        "            epoch, loss_ / len(train_loader), mae.item(), mre.item()), f, params[\"run_logger\"])\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "        # Run Validation\n",
        "        resnet.eval()\n",
        "        _, _, _, val_loss = do_eval(resnet, val_loader, epoch)\n",
        "        lr_scheduler.step(val_loss)\n",
        "        gc.collect()\n",
        "        torch.save({\n",
        "            'epoch': e,\n",
        "            'model_state_dict': resnet.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': lr_scheduler.state_dict()\n",
        "        }, os.path.join(params[\"save_path\"], \"MODELS\") + '/%s/Tops_ResNet_blocks_3_model_epoch_%d' % (\n",
        "            params[\"expt_name\"], epoch))\n",
        "\n",
        "        if params[\"run_logger\"]:\n",
        "            f.close()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0qeF23NbnQ-"
      },
      "source": [
        "## Starting the Training Process\n",
        "Let's start by defining the hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiLLxbSjVzKJ"
      },
      "source": [
        "params={\n",
        "  \"batch_size\": 1024,\n",
        "  \"epochs\": 50,\n",
        "  \"load_epoch\": 0,\n",
        "  \"lr\": 1e-3,\n",
        "  \"resblocks\": 3,\n",
        "  \"input_channels\": 3,\n",
        "  \"fmaps\": [\n",
        "    16,\n",
        "    32\n",
        "  ],\n",
        "  \"is_cuda\": 1,\n",
        "  \"run_logger\": 1,\n",
        "  \"expt_name\": \"TopGun_scaled-target&input-500-0.02-0.2-1_lr_scheduled-1e-3\",\n",
        "  \"save_path\": \".\",\n",
        "  \"data_path\": \".\",\n",
        "  \"channel1_scale\": 0.02,\n",
        "  \"channel2_scale\": 0.2,\n",
        "  \"channel3_scale\": 1.0,\n",
        "  \"seed\": 0\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mKU1oMW_EKp"
      },
      "source": [
        "np.random.seed(params[\"seed\"])\n",
        "torch.manual_seed(params[\"seed\"])\n",
        "random.seed(params[\"seed\"])\n",
        "\n",
        "LOG_PATH = os.path.join(params[\"save_path\"], \"LOGS\")\n",
        "MODEL_PATH = os.path.join(params[\"save_path\"], \"MODELS\")\n",
        "PLOT_PATH = os.path.join(params[\"save_path\"], \"PLOTS\")\n",
        "\n",
        "if not os.path.isdir(LOG_PATH):\n",
        "    os.makedirs(LOG_PATH)\n",
        "for d in [MODEL_PATH, PLOT_PATH]:\n",
        "    if not os.path.isdir('%s/%s' % (d, params[\"expt_name\"])):\n",
        "        os.makedirs('%s/%s' % (d, params[\"expt_name\"]))\n",
        "\n",
        "train_loader, val_loader, test_loader = train_val_loader(\n",
        "    [os.path.join(params[\"data_path\"], f) for f in\n",
        "     os.listdir(params[\"data_path\"])], params[\"batch_size\"])\n",
        "\n",
        "resnet = ResNet(params[\"input_channels\"], params[\"resblocks\"], params[\"fmaps\"])\n",
        "\n",
        "if params[\"is_cuda\"]:\n",
        "    resnet.cuda()\n",
        "\n",
        "optimizer = optim.Adam(resnet.parameters(), lr=params[\"lr\"])\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "train(resnet, optimizer, lr_scheduler, params[\"epochs\"], train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrS_HHyWpanu"
      },
      "source": [
        "## Evaluating on Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocBQbIyXmmBC"
      },
      "source": [
        "np.random.seed(params[\"seed\"])\n",
        "torch.manual_seed(params[\"seed\"])\n",
        "random.seed(params[\"seed\"])\n",
        "\n",
        "LOG_PATH = os.path.join(params[\"save_path\"], \"LOGS\")\n",
        "MODEL_PATH = os.path.join(params[\"save_path\"], \"MODELS\")\n",
        "PLOT_PATH = os.path.join(params[\"save_path\"], \"PLOTS\")\n",
        "\n",
        "if not os.path.isdir(LOG_PATH):\n",
        "    os.makedirs(LOG_PATH)\n",
        "for d in [MODEL_PATH, PLOT_PATH]:\n",
        "    if not os.path.isdir('%s/%s' % (d, params[\"expt_name\"])):\n",
        "        os.makedirs('%s/%s' % (d, params[\"expt_name\"]))\n",
        "\n",
        "train_loader, val_loader, test_loader = train_val_loader(\n",
        "    [os.path.join(params[\"data_path\"], f) for f in\n",
        "     os.listdir(params[\"data_path\"])], params[\"batch_size\"])\n",
        "\n",
        "resnet = ResNet(params[\"input_channels\"], params[\"resblocks\"], params[\"fmaps\"])\n",
        "\n",
        "if params[\"is_cuda\"]:\n",
        "    resnet.cuda()\n",
        "\n",
        "optimizer = optim.Adam(resnet.parameters(), lr=params[\"lr\"])\n",
        "\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "if load_epoch != 0:\n",
        "        model_name = 'Tops_ResNet_blocks_3_model_epoch_%d'%(params[\"load_epoch\"])\n",
        "        resnet, optimizer,lr_scheduler = load_model(model_name, resnet, optimizer,lr_scheduler)\n",
        "resnet.eval()\n",
        "mre,m_pred,m_true,mean_mae=do_eval(resnet, val_loader, params[\"load_epoch\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP2ueRG6amcm"
      },
      "source": [
        "df=pd.DataFrame()\n",
        "df[\"m_pred\"]=m_pred[:,0]\n",
        "df[\"mre\"]=mre[:,0]\n",
        "df[\"m_true\"]=m_true[:,0]\n",
        "df_select=df[df[\"mre\"]>0.5]\n",
        "plt.title(\"Percentage of samples with mre bigger than 50%%: %.4f%%\"%(df_select.shape[0]*100/df.shape[0]))\n",
        "plt.scatter(df_select[\"m_true\"],df_select[\"mre\"])\n",
        "plt.xlabel(\"Target Mass\")\n",
        "plt.ylabel(\"MRE\")\n",
        "plt.savefig(os.path.join(params[\"save_path\"], \"PLOTS\") +'%s/mpred_massvsmre_val.png'%(params['expt_name']), bbox_inches='tight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8aosTmupjTP"
      },
      "source": [
        "## Evaluating on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLmTT3dGpol5"
      },
      "source": [
        "np.random.seed(params[\"seed\"])\n",
        "torch.manual_seed(params[\"seed\"])\n",
        "random.seed(params[\"seed\"])\n",
        "\n",
        "LOG_PATH = os.path.join(params[\"save_path\"], \"LOGS\")\n",
        "MODEL_PATH = os.path.join(params[\"save_path\"], \"MODELS\")\n",
        "PLOT_PATH = os.path.join(params[\"save_path\"], \"PLOTS\")\n",
        "\n",
        "if not os.path.isdir(LOG_PATH):\n",
        "    os.makedirs(LOG_PATH)\n",
        "for d in [MODEL_PATH, PLOT_PATH]:\n",
        "    if not os.path.isdir('%s/%s' % (d, params[\"expt_name\"])):\n",
        "        os.makedirs('%s/%s' % (d, params[\"expt_name\"]))\n",
        "\n",
        "train_loader, val_loader, test_loader = train_val_loader(\n",
        "    [os.path.join(params[\"data_path\"], f) for f in\n",
        "     os.listdir(params[\"data_path\"])], params[\"batch_size\"])\n",
        "\n",
        "resnet = ResNet(params[\"input_channels\"], params[\"resblocks\"], params[\"fmaps\"])\n",
        "\n",
        "if params[\"is_cuda\"]:\n",
        "    resnet.cuda()\n",
        "\n",
        "optimizer = optim.Adam(resnet.parameters(), lr=params[\"lr\"])\n",
        "\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "if load_epoch != 0:\n",
        "        model_name = 'Tops_ResNet_blocks_3_model_epoch_%d'%(params[\"load_epoch\"])\n",
        "        resnet, optimizer = load_model(model_name, resnet, optimizer,lr_scheduler)\n",
        "resnet.eval()\n",
        "mre,m_pred,m_true,mean_mae=do_eval(resnet, val_loader, params[\"load_epoch\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt38kx2ou5OF"
      },
      "source": [
        "df=pd.DataFrame()\n",
        "df[\"m_pred\"]=m_pred[:,0]\n",
        "df[\"mre\"]=mre[:,0]\n",
        "df[\"m_true\"]=m_true[:,0]\n",
        "df_select=df[df[\"mre\"]>0.5]\n",
        "plt.title(\"Percentage of samples with mre bigger than 50%%: %.4f%%\"%(df_select.shape[0]*100/df.shape[0]))\n",
        "plt.scatter(df_select[\"m_true\"],df_select[\"mre\"])\n",
        "plt.xlabel(\"Target Mass\")\n",
        "plt.ylabel(\"MRE\")\n",
        "plt.savefig(os.path.join(params[\"save_path\"], \"PLOTS\") +'%s/mpred_massvsmre_test.png'%(params['expt_name']), bbox_inches='tight')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
